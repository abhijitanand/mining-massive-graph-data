\documentclass{scrartcl}

\usepackage{tikz}
\usepackage{booktabs}
\usepackage{paralist} \usepackage {subfigure} \usepackage {graphicx}
\usepackage{amsfonts, algorithm, algorithmic, graphicx, subfigure, tabularx, amssymb, amsmath, rotating, multirow}
\usepackage[show]{notes-alt}

%% Custom Commands
% \usepackage{times}
\usepackage{xspace}
\newcommand{\aanand}[1]{\textcolor{blue}{\textit{#1}}}
\newcommand{\comment}{}
\newcommand{\hide}[1]{}
\usepackage{xspace}
\newcommand {\N}{\mathcal{N}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\pagenumbering{arabic}

\begin{document}

  
% \title{Querying and Mining Graph Data Streams}
 \title{Large scale matching for Graph Data Streams}
 \maketitle

\section{Introduction}

Graphs encode the interconnections between entities to model relations between them.They have proven to be an intuitive datastructure in modelling, querying and reasoning about real-life relationships. In Web search the graphs derived from links between webpages are used for link analysis and establishing authority of webpages. In social networks, the graph structure captures interesting insights about the influential nodes, community structures, connectivity and reachability etc. In biological networks people are interested in how diseases spread. In ontologies, they are useful to model taxonomies in the form of type hierarchies. Apart from these scenarios they find applications in road, water networks for shortest path computations and flow problems.

Traditional research has focussed on largely on \emph{static graphs}. However, as one might quickly realize, most of the scenarios enumerated above have a evolutionary nature to them. Social networks, Web graphs, citation networks and even biological networks evolve over time. So there has been a growing need to support data management, querying, mining and analytics tasks over evolving or time-varying graphs.  Moreso, to date there is little work on understanding the challenges needed to ingest, manage and query such data in large graph databases. 


Streaming~\cite{Henzinger,Feigenbaum, Alon} is an important model for computation of massive graphs. This model usually constraints the amount of memory that can be used for the processing. The aim in this model usually is to use (poly) $\log$ space but general graph problems like matching and connectivity are considered hard in (poly) $\log$ space. To tackle such problems a semi streaming model has been suggested where the algorithms can use $O(n \text{ poly} \log n)$ bits of space. In addition the semi streaming model allows multiple passes over the data.  We further relax this model to propose a 
 propose a new streaming model in which we fix the time complexity and study the approximations for graph problems while varying the space complexity. 
To understand this new model consider the \emph{greedy} algorithm for finding maximum cardinality matching where each edge is included in the matching only if neither of its end vertices are in the matching. At present this is the best single pass algorithm providing $2$ approximation when the edges are streamed in an arbitrary order. It uses space $O(n)$ and is a single pass algorithm , i.e., it scans the input data sequentially just once. The approximation factor can be improved by improving the number of passes, i.e. increasing the number of times each edge is processed. In our new model the is to fix the time complexity, say to linear in the number of total edges which then implies an amortized $O(1)$ time to process each edge. The efficiency of the algorithms would then be judged by the approximation factor of the result and the space complexity.  


Recently, a semi streaming model has been suggested where the algorithms can use $O(n \text{ poly} \log n)$ bits of space. In addition the semi streaming model allows multiple passes over the data.  In this work we propose a new streaming model in which we fix the time complexity and study the approximations for graph problems while varying the space complexity. To understand this new model consider the \emph{greedy} algorithm for finding maximum cardinality matching where each edge is included in the matching only if neither of its end vertices are in the matching. At present this is the best single pass algorithm providing $2$ approximation when the edges are streamed in an arbitrary order. It uses space $O(n)$ and is a single pass algorithm , i.e., it scans the input data sequentially just once. The approximation factor can be improved by improving the number of passes, i.e. increasing the number of times each edge is processed. In our new model the is to fix the time complexity, say to linear in the number of total edges which then implies an amortized $O(1)$ time to process each edge. The efficiency of the algorithms would then be judged by the approximation factor of the result and the space complexity.  


\section{Alternate Introduction}
Graphs encode the interconnections between entities to model relations between them.They have proven to be an intuitive datastructure in modelling, querying and reasoning about real-life relationships. They have been heavily used in link analysis for Web search, social networks analysis, community detection, biological networks and in knowlege representation. There is however two important characteristics of such real world graphs. First they are now progressively become \emph{massive} and secondly, most of them evolve with time making them \emph{dynamic}. For large graph data management most of the work has focussed on either \emph{static graphs} or \emph{streaming graphs}.

Streaming is an important model for computation of massive graphs. This model usually constraints the amount of memory that can be used for the processing. Also, it typically prohibits more than a single pass over the input data. Apart from their application in pure streaming based application, the attractiveness of the streaming model stems from the fact that even large static collections could be processed in reasonable amounts time. However, one of the limitations of such streaming algorithms is that some fundamental graph problems are known to be hard in such a contrained setup. Specifically, the space requirements in the streaming model is (poly) $\log$ of the input but general graph problems like matching and connectivity are considered hard in (poly) $\log$ space. We operate on the observation that for scenarios when the input is not strictly streaming, e.g., large static graphs or dynamic graphs with a slow change rate one can relax the strict memory and running-time assumptions to realize tractable solutions for the otherwise hard graph algorithms.

Recently, a semi streaming model has been suggested where the algorithms can use $O(n \text{ poly} \log n)$ bits of space. In addition the semi streaming model allows multiple passes over the data.  In this work we propose a new streaming model in which we fix the time complexity and study the approximations for graph problems while varying the space complexity. To understand this new model consider the \emph{greedy} algorithm for finding maximum cardinality matching where each edge is included in the matching only if neither of its end vertices are in the matching. At present this is the best single pass algorithm providing $2$ approximation when the edges are streamed in an arbitrary order. It uses space $O(n)$ and is a single pass algorithm , i.e., it scans the input data sequentially just once. The approximation factor can be improved by improving the number of passes, i.e. increasing the number of times each edge is processed. In our new model the is to fix the time complexity, say to linear in the number of total edges which then implies an amortized $O(1)$ time to process each edge. The efficiency of the algorithms would then be judged by the approximation factor of the result and the space complexity.  

%In this proposal we attempt to scope out the problems relating to dynamic graphs and more specifically time-varying graphs with an intent to identify open issues and problems which would be worth investigating.

%\todo{Certain examples neededlike shortest paths, k core decomposition, matchings}
%
%\section{State of the Art and Preliminary Work}
%
%We divide the related work survey into two sections: (a) algorithmic foundations on streaming graph primitives, and (b) indexing and querying for streaming graphs.
%
%\subsection{State of the Art}
\subsection{Matching in Graphs}
A fundamental problem in graphs is finding a maximum cardinality matching. Given a graph $G$, a matching $M$ in $G$ is a set of pairwise non-adjacent edges; that is, no two edges share a common vertex.
Two natural variants of this problem in the streaming scenario have been considered in the literature: (1) the edge
arrival setting, where edges arrive in the stream and (2) the vertex arrival setting, where vertices on one
side of the graph arrive in the stream together with all their incident edges.
The latter setting has also
been studied extensively in the context of online algorithms, where each arriving vertex has to either be
matched irrevocably or discarded upon arrival.

The basic algorithms for finding matchings rely on the definition of augmenting paths:
given a matching $M$ in the graph, an augmenting path is an odd-length
(simple) path with its edges alternating between being in $M$
and not, and with the two end edges not in $M$. In order to increase the size of the matching, one starts with an unmatched vertex and walks along the augmenting path to reach another unmatched vertex. The unmached edges of the previous matching now are the matched edges and vice versa.

One can employ various methods to find these augmenting paths; namely breadth first search (BFS), depth first search (DFS) and random walk. The BFS and DFS approaches require the full knowledge of the graph and hence are not ideal for streaming models. The random walk method on the one hand chooses each vertex on the augmenting path randomly and is a stateless approach but on the other hand has no gurantees of the convergence in the worst case.
The other class of algorithms employed for finding online matchings is \emph{greedy} approach where each edge is included in the matching only if neither of its end vertices are in the matching. At present this is the best single pass algorithm providing $2$ approximation when the edges are streamed in an arbitrary order. The strongest known lower bound for the single pass algorithms is $e/(e-1)\approx 1.58$ which also applies when the edges are grouped by end point~\cite{boundsKap,Goel}.

In the next section we describe  a matching algorithm for sparse random bipartite graphs and argue about the possible extensions to more general graph models. 


\subsubsection{Previous Work}
We consider a bipartite graph $G(U,V,E)$, in which $U$ is known to the algorithm, vertices in $V$ are
unknown, but arrive one at a time, revealing the edges incident on
them as they arrive. In addition each vertex in $U$ has $k$ random incident edges and the other end of such edges are chosen uniformly randomly and independently. The algorithm has to match a vertex
as soon as it arrives. This graph model is motivated from a class of load balancing problems in which we have a set of machines and jobs where each job chooses $k$ random machines and needs to be assigned to one of them.

In \cite{kCores} we compute a threshold such that when the density of $G$ is below that threshold a (left-) perfect matching (i.e. all jobs are assigned to one of their choices) exists, otherwise not.
%We consider a multiple choice scenario in which we have $n$ machines, $m$ jobs and each job needs to be assigned to one of its $k$ random choices. In addition each bin has capacity $\ell$.
%\note{Shall I also explain the applications like cuckoo hashing and data management here ?}
%The most important question to answer here is that under what conditions such an assignment is possible. 
%In \cite{kCores} we compute a threshold, $c$ such that when the ratio of the number of jobs to that of bins is below $c$ there exists an assignment, otherwise not. The model that we consider above can been seen as a bipartite graph $G$ which then translates our result to computing thresholds for the existence of a perfect matching in $G$.
%
%More precisely 

In~\cite{ballsbins} we propose an algorithm which we call \emph{local search allocation} finds maximum cardinality matching in linear time with high probability.
Our approach in~\cite{ballsbins} operates by assigning labels to vertices in $V$. The vertices for augmenting path ae chosen based on their labels. The labels are so updated such that at any time the label of a vertex is atmost the shortest distance to an unmatched vertex. In addition to the efficiency of the approach, the algorithm never runs in loops and gives the information when the vertex cannot be matched. The space complexity of the algorithm is $O(|E|)$. This approach is different from the greedy approach which is commonly employed in such a model in the sense that it allows the matching to change as and when necessary. This means that once an edge is seleected for the matching, it can be kicked out later on as and when required.

Even though this algorithm has been designed for a more specific class of bipartite graphs, it can be applied directly on more general bipartite graphs. We focus on the following questions.

With (i) $O(n \text{ poly} \log n)$ space and  (ii) amortized $O(1)$ time processing of each edge
\begin{itemize}
\item  can we achieve an approximation ration closer to the known lower bound $(e/(e-1))$ for general bipartite graphs ?
\item what approximation ratios can we achieve with high probability on different models of random graphs like uniformly random or power law graphs ?
\item what approximations ration can we achieve for weighted matchings?

\end{itemize}

%\todo{
%what we did ? How our work would be used in this project
%for example :  
%\begin{itemize}
%	\item (Megha ) Studied  k cores , dense components, matching
%	\item(Avishek) :  
%\end{itemize}
%
%}
%\note{state bounds for maximum cardinality matchings in bipartite graphs and general graphs
%explain greedy algorithm ... variants of greedy algorithm improving bounds..not much improvement theretically
%Introduce local search allocation for online setting....the model is different from the above considered models...a new idea based on approximate augmenting path lengths ; simple to implement 
%scope to adapt the algorithm to general graphs ; there are of course problems in breaking the symmetry}

%\subsubsection {Thresholds for a Perfect Matching}
%In \cite{kCores} we estimate the threshold for the density of the random uniform hypergraph when the k core first appears. Such a threshold is associated with the probability of existence of perfect matchings in the associated bipartite graphs. Such estimates are quite useful .... 


%
%
%In~\cite{ballsbins} we propose an algorithm which we call \emph{local search allocation} finds maximum cardinality matching in linear time with high probability.
%Our approach in~\cite{ballsbins} operates by assigning labels to vertices in $V$. The vertices for augmenting path ae chosen based on their labels. The labels are so updated such that at any time the label of a vertex is atmost the shortest distance to an unmatched vertex. In addition to the efficiency of the approach, the algorithm never runs in loops and gives the information when the vertex cannot be matched. Emperical evidence suggests that our approach performs an order of magnitude better than the random walk method.

%\subsubsection {Matching in a Streaming Model}
%
%%matching is core to resource allocation problems of various types from the scheduling and Operations Research literature, for example, allocating jobs to machines in cloud computing
% 
%
%
%In the model that we consider, we are given a bipartite graph $G(U,V,E)$, in which $U$ is known to the algorithm, vertices in $V$ are
%unknown, but arrive one at a time, revealing the edges incident on
%them as they arrive. The algorithm has to match a vertex
%as soon as it arrives. 
%In addition each vertex in $V$ has $k$ incident edges and the other end of such edges are chosen uniformly randomly and independently. This model is motivated from a class of multiple choice load balancing problems in which each job ( $u\in U$) chooses $k$ machines uniformly randomly and independently. 
%
%In \cite{kCores} we prove that there exists a threshold such that when the density of $G$ is below that threshold a (left-) perfect matching (i.e. all jobs are assigned to one of their choices) exists, otherwise not. 


%
%\subsubsection{Proposed Research}
%
%As a starting point our proposed research focusses on the following questions.
%
%\begin{enumerate}
%\item Can we extend/adapt local search allocation algorithm to find approximate maximum cardinality matchings in the streaming model  in which a bipartite graph is given by a stream of edges in an arbitrary order? Such a semi streaming algoritm has memory $O(n~ \text{polylog} ~n)$, and the graph vertices are known before processing the stream of edges.
%\item 
%\end{enumerate}
%
%
%
%we aim to develop new algorithms based on local search method proposed in .... for computing aproximate matchings for online model



\subsection{Indexing and Query Processing}

Within the streaming model, there are important applications of matching algorithms like \emph{internet advertisements}, \emph{load balancing in cloud computing}, \emph{switch scheduling} etc. Based on the research problems in the previous section, we now turn our attention to how to utilize those results into building indexing frameworks to efficiently answer \emph{matching-based queries} or simply \emph{matching queries}. Indexing large graphs to support primitive operations like reachability~\cite{seufert2013ferrari}, shortest paths~\cite{Gubichev:2010}, dense subgraphs~\cite{angel_dense_2013} have been explored in the graph databases literature with reasonable success.  As an example, a specialized index can be created over an input graph to answer \emph{reachability queries}, i.e., if a pair of nodes are reachable from each other. Similarly, we intend to design efficient indexing methods for streaming graph input to find matchings. Similarly for massive graphs, matchings for arbitrary subgraphs can be efficiently computed if some kind of an index is constructed over the input graph.

\subsubsection{Matching-based Queries} 
We consider two types of queries : \emph{standing matching queries} and \emph{adhoc matching queries}. Standing queries (also called as publisher-subscriber queries) are a static set of queries which are checked for satisfaction in the face of changing or dynamic input data. Examples are dense-subgraph queries proposed by Angel et. al.~\cite{angel_dense_2013} which reports dense subgraphs whenever streaming edges result in a subgraph becoming substantially dense. The query in that case is a desired density coefficient. In our scenario, we could also be interested in potentially new matchings given a stream of incoming edges/nodes. The second class of queries are the more standard adhoc queries, where user can desire a given type of matching on a given subgraph. As an example, for time-varying graphs a user could be interested in matchings for the graph which was valid in a given time interval.


\subsubsection{Research Questions} 	
In this respect we can formulate the following research questions which we intend to answer:
\begin{itemize}
	\item \textsf{[RQ I]} How can we \emph{efficiently construct} indexes specialized for answering matching queries ? 

	\item \textsf{[RQ II]} What kind of index-maintenance strategies is needed to be employed to avoid partial or complete index recomputations ?

	\item \textsf{[RQ II]} What query processing techniques need to be employed for both \emph{standing} and \emph{adhoc} matching queries ?

\end{itemize}



%\subsubsection {Temporal Graphs }
%
%\subsection{Project Related Publications}
%
%\section{Objectives and Work Programme}
%\subsection{Anticipated Total Duration of the Project}
%\subsection{Objectives}
%\subsection{Work Programme including Proposed Research Methods}
%\subsection{Data Handling}
%\subsection{Other Information}
%\subsection{Explanations on the Proposed Investigations}
%\subsection{Information on the Scientific and Financial Involvement of International Cooperation Partners}

\bibliographystyle{plain}
\bibliography{proposal-outline}

%General intro to graphs and contemporary application domains
 

% Evolving graphs and need for scoping out the area



%\input{temporal-graphs}

%\section{Open Problems}


%\section{Seminal Works}


%\section{Application Areas}

\end{document}

